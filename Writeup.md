Model Narrative

Problem and novelty: The task is modernizing Early Modern English into contemporary wording. The “novel” aspect is the side-by-side build of (1) a noisy-channel system whose channel model is enriched with learned word-level bonuses and context-sensitive edits, and (2) a character-level Transformer baseline trained on exactly the same data splits. This lets us contrast an interpretable probabilistic approach against an end-to-end neural one on identical inputs.

Probabilistic path (steps 2, 3a, 3b): The noisy-channel model (`scripts/train.py`) combines a 5-gram character language model (`language_model.py`) for fluency with an enhanced channel model (`channel_model.py`) that learns character insert/delete/substitute costs, word-level mappings, and context-sensitive substitutions. Decoding (`noisy_channel.py`) scores `P(modern | early) ∝ P(modern) × P(early | modern)` but keeps search tractable via heuristic candidates (identity, learned word replacements, frequent char reversals, lowercasing, and combinations) and greedy selection. It is trained separately on synthetic data (step 3a) and optionally on processed “real” data (step 3b), saving bundles to `models/synthetic_model.pkl` and `models/processed_model.pkl`. Evaluation (`scripts/evaluate.py`) reports exact match and character error rate; on synthetic test data it reaches 22% exact-match accuracy and 0.103 CER (`results/synthetic_on_synthetic.txt`), with errors concentrated where the learned edit lexicon lacks coverage (e.g., “do” vs. “dost”).

Neural path (steps 2, 3a, 3b): The character-level Transformer (`scripts/train_transformer.py`) treats the same pairs as seq2seq, encoding early text and decoding modern text with configurable layers/heads/hidden sizes, sinusoidal positional encodings, and label smoothing. Everything runs on character vocabularies with BOS/EOS/PAD/UNK tokens; decoding is greedy but fully learned—no hand-crafted candidate rules. Checkpoints land at `models/transformer_<data>.pt` (e.g., `models/transformer_synthetic.pt`). `scripts/evaluate_transformer.py` reloads a checkpoint and scores held-out splits; on synthetic test data it achieves 85% exact match and 0.0107 CER (`results/transformer_synthetic_on_synthetic_test.txt`), showing much higher fidelity on the synthetic distribution.

Pros, cons, and resource notes (step 4): The noisy-channel route is data-light, transparent, and cheap to train; you can inspect edit counts and word-mapping frequencies to see why a decision was made. Its downside is limited rewrite capacity and brittleness when the candidate generator misses a pattern. The Transformer is data- and compute-heavier and less interpretable, but it learns richer rewrites and generalizes better, as seen in the synthetic results. Real-data training (step 3b) is supported by both pipelines via the `processed` splits; those runs were not executed yet, so qualitative/quantitative comparisons on real text remain open. A single table of results (to satisfy the project guidance) would include at minimum: synthetic noisy-channel (22% / 0.103 CER) and synthetic Transformer (85% / 0.0107 CER), with rows reserved for processed-data runs once they are produced.

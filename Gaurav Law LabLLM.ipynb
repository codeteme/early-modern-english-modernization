{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzQWnezCcJpo"
   },
   "source": [
    "# Finetune GPT-2 on wiki-text\n",
    "\n",
    "In this Lab, we are using a series of library from Hugging Face (i.e. tranformers, datasets, peft). You may need to go through the document of these library to learn the usage. (Hint: you may use the imported contents in the code cell below, other contents is not necessary for this lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nuF3wODvcJps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import inspect\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by me to fix cuda memory issue\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eqewntncJps"
   },
   "source": [
    "## Lab 2(a) Generate text with GPT2\n",
    "\n",
    "Using the API provided by hugging face, we can easily load the pre-trained GPT2 model and generate text. (GPT2 is a early generative model, the quality of the generated text is not as good as the later model like GPT3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TGrZm5H1cJps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is a language model based on transformer developed by OpenAI and developed in collaboration with the AI Research Institute at the University of Arizona.\n",
      "\n",
      "OpenAI is a project of the Center for Artificial Intelligence, University of Phoenix, Arizona, USA.\n",
      "\n",
      "OpenAI is published by the Center for Artificial Intelligence, University of Phoenix, Arizona, USA.\n"
     ]
    }
   ],
   "source": [
    "# your code here: load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length):\n",
    "\n",
    "\n",
    "    # your code here: tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors = \"pt\", padding = True).to(model.device)\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    # your code here: generate token using the model\n",
    "    gen_tokens = model.generate(input_ids = input_ids,\n",
    "                               attention_mask = attention_mask,\n",
    "                               max_length = max_length,\n",
    "                               do_sample = True,\n",
    "                               top_p = 0.9,\n",
    "                               top_k = 50,\n",
    "                               temperature = 0.8,\n",
    "                               pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "    # your code here: decode the generated tokens\n",
    "    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens = True)\n",
    "    \n",
    "    print(gen_text)\n",
    "\n",
    "generate_text(model, tokenizer, \"GPT-2 is a language model based on transformer developed by OpenAI\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIDadMnEcJpt"
   },
   "source": [
    "## Lab 2(b) Prepare dataset for training\n",
    "\n",
    "Please fill the code cell below to download the dataset and prepare the dataset for finetuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ggwmYzDycJpt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([4, 1024])\n",
      "DataLoader is working correctly!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# your code here: load the dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# get 10% of dataset\n",
    "dataset_train = dataset[\"train\"].select(range(len(dataset[\"train\"]) // 10))\n",
    "dataset_valid = dataset[\"validation\"].select(range(len(dataset[\"validation\"]) // 10))\n",
    "\n",
    "# your code here: implement function that tokenize the dataset and set labels to be the same as input_ids\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"],\n",
    "                         padding = \"max_length\",\n",
    "                         truncation = True,\n",
    "                         max_length = 1024)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# your code here: tokenize the dataset (you may need to remove columns that are not needed)\n",
    "tokenized_datasets_train = dataset_train.map(tokenize_function, batched = True, remove_columns = dataset_train.column_names)\n",
    "tokenized_datasets_valid = dataset_valid.map(tokenize_function, batched = True, remove_columns = dataset_train.column_names)\n",
    "\n",
    "tokenized_datasets_train.set_format(\"torch\")\n",
    "tokenized_datasets_valid.set_format(\"torch\")\n",
    "\n",
    "# your code here: create datacollator for training and validation dataset\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False, pad_to_multiple_of = None)\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets_train, shuffle=True, batch_size=4, collate_fn=data_collator)\n",
    "valid_dataloader = DataLoader(tokenized_datasets_valid, batch_size=4, collate_fn=data_collator)\n",
    "\n",
    "# Test the DataLoader\n",
    "for batch in train_dataloader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['attention_mask'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break\n",
    "\n",
    "print(\"DataLoader is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0D6QvVucJpt"
   },
   "source": [
    "## Lab 2(c) Evaluate perplexity on wiki-text\n",
    "\n",
    "Before finetuning, we evaluate the pre-trained GPT2 model on the wiki-text dataset. The perplexity is a common metric to evaluate the performance of language model. The lower the perplexity, the better the model. To compute the perplexity in practice, we use the formula as follows, which is a transformation of the formula in class:\n",
    "$PP(W) = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i|\\text{context})\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u1eLOb48cJpt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial perplexity: 42.9958610534668\n"
     ]
    }
   ],
   "source": [
    "def evaluate_perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_length = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # your code here: get the input_ids, attention_mask, and labels from the batch\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "            labels = batch['labels'].to(model.device)\n",
    "\n",
    "            # your code here: forward pass\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask, labels = labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            # your code here: calculate the loss\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                           shift_labels.view(-1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_length += attention_mask.sum().item()\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_length))\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "perplexity = evaluate_perplexity(model, valid_dataloader)\n",
    "print(f\"Initial perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKiP9TvhcJpu"
   },
   "source": [
    "## Lab 2(d) Fine-tune GPT2 on wiki-text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O5j4XHrPchSg"
   },
   "outputs": [],
   "source": [
    "# Tip: Print out transformer version and training arguments\n",
    "# print(\"transformers version:\", transformers.__version__)\n",
    "# print(\"TrainingArguments signature:\")\n",
    "# print(inspect.signature(TrainingArguments.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ypYhPhTicJpu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3738273/709158199.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model = model,\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 34:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.436400</td>\n",
       "      <td>3.341595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.131600</td>\n",
       "      <td>3.368648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.890600</td>\n",
       "      <td>3.394768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-wikitext-2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    prediction_loss_only=True,\n",
    "    \n",
    "    # your code here: report validation and training loss every epoch\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_steps = 50,\n",
    "    report_to = None,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "# print(\"transformers version:\", transformers.__version__)\n",
    "# print(\"TrainingArguments signature:\")\n",
    "# print(inspect.signature(TrainingArguments.__init__))\n",
    "\n",
    "# your code here: create a Trainer object\n",
    "trainer = Trainer(model = model,\n",
    "                  args = training_args,\n",
    "                  tokenizer = tokenizer,\n",
    "                  data_collator = data_collator,\n",
    "                  train_dataset = tokenized_datasets_train,\n",
    "                  eval_dataset = tokenized_datasets_valid)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBaoWG6HcJpu"
   },
   "source": [
    "# Test fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wXufraCqcJpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuned perplexity: 25.921186447143555\n"
     ]
    }
   ],
   "source": [
    "# your code here: load the fine-tuned model\n",
    "model_finetuned = AutoModelForCausalLM.from_pretrained(\"gpt2-wikitext-2\").to(device)\n",
    "perplexity = evaluate_perplexity(model_finetuned, valid_dataloader)\n",
    "print(f\"fine-tuned perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uPdNkhycJpu"
   },
   "source": [
    "# Generate some text using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "N_tDKN8fcJpv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is a language model based on transformers developed by OpenAI . It is a simple and efficient mechanism for generating new combinations of monadic and non-adic languages . The monadic and non-adic features of the language can be expressed using the new features of the monadic language . The monadic features of the language can be expressed using the new features of the non-adic language . The monadic features can be expressed using the new features of the non-adic language .\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# generate text\n",
    "generate_text(model_finetuned, tokenizer, \"GPT-2 is a language model based on transformers developed by OpenAI\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSHq0mMCcJpv"
   },
   "source": [
    "## Lab 2(e) Parameter efficient fine-tuning (LoRA)\n",
    "\n",
    "finetune the base gpt model through LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dSgEy6w3cJpv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3738273/4192233448.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model = model_lora,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after lora finetuning: 25.921186447143555\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.2,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# your code here: load GPT2 model and add the lora adapter\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_lora = get_peft_model(model, peft_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-lora-wikitext-2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    prediction_loss_only=True,\n",
    "    \n",
    "    # your code here: report validation and training loss every epoch\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_steps = 50,\n",
    "    report_to = None,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "# your code here: set trainer and train the model\n",
    "trainer = Trainer(model = model_lora,\n",
    "                  args = training_args,\n",
    "                  tokenizer = tokenizer,\n",
    "                  data_collator = data_collator,\n",
    "                  train_dataset = tokenized_datasets_train,\n",
    "                  eval_dataset = tokenized_datasets_valid)\n",
    "\n",
    "ppl = evaluate_perplexity(model_lora, valid_dataloader)\n",
    "print(f\"Perplexity after lora finetuning: {ppl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHhwptfQcJpv"
   },
   "source": [
    "# Evaluate lora fine-tuned model on wiki-text\n",
    "\n",
    "compare the text generated by the fully fine-tuned model and LoRA fine-tuned model and the pre-trained model. Do you see any difference in the quality of the generated text? Try to explain why. (Hint: trust your result and report as it is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wT_v_vMccJpv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is a language model based on transformers developed by OpenAI for the production of a more efficient and efficient language . \n",
      "\n",
      "OpenAI is not a \" new \" language , nor is it a new language for language engineering . \n",
      "\n",
      "Instead, it is a new system , and it uses a different approach to learning . \n",
      "\n",
      "The goal of this approach is to create a more efficient language and improve it at a faster rate . \n",
      "\n",
      "There are two main\n"
     ]
    }
   ],
   "source": [
    "generate_text(model_lora, tokenizer, \"GPT-2 is a language model based on transformers developed by OpenAI\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIdFcSDkcJpv"
   },
   "source": [
    "Compare the perplexity of the fully fine-tuned model and LoRA fine-tuned model. Do you see any difference in the perplexity? Try to explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "L13GwQm7cJpv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after lora finetuning: 25.921186447143555\n"
     ]
    }
   ],
   "source": [
    "ppl = evaluate_perplexity(model_lora, valid_dataloader)\n",
    "\n",
    "print(f\"Perplexity after lora finetuning: {ppl}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
